defaults:
  - _self_

model:
  sbert_name: "sentence-transformers/all-MiniLM-L6-v2"
  attention_augment: False
  fourier:
    use: False
    mode: "lowpass"
    keep_ratio: 0.5
  loss:
    l_comp: 0.1       # weight for adversary loss
    l_s: 0.01         # sparsity regularizer
    l_tv: 10.0        # total variation regularizer
    tau: 0.07         # InfoNCE temperature (was model.nt_xent.temperature)
    use_null_target: False  # Push complement embeddings toward the encoded null vector
  dual:
    use: False           # Use two rationale models
    num_models: 2        # Number of rationale models when multi-model mode is enabled
    kl_weight: 1.0      # Weight for KL divergence loss
    sparsity_weights: [5.0, 1.0]  # Optional explicit per-model sparsity weights
  optim:
    lr: 5e-5
    weight_decay: 1e-2
    lr_adv: 5e-5
    weight_decay_adv: 1e-2
    betas: [0.9, 0.999]
  clustering:
    use: False
    show_details: False
    proposal_thresh: 0.5
    num_clusters: 2
    max_tokens: -1
    max_fit_batches: 50
    iters: 25
    tol: 1e-4
    seed: 42

data:
  rebuild_ds: False
  train:
    dataset: "cnn"
    batch_size: 32
    num_workers: 4
    subset: 1.0
    shuffle: True
  eval:
    dataset: "wikiann"
    batch_size: 32
    num_workers: 1
    subset: 1.0
    shuffle: True

train:
  retrain: True
  epochs: 10
  grad_clip: 1.0

eval:
  eval_only: False
  verbose: False
  thresh: 0.5
  reference_sentence:
    use: False
    sentence: "On 14 July 2019, Dr. Amelia Johnson met UN Secretary-General Ant√≥nio Guterres in Paris to discuss AI ethics with Google, Microsoft, and IBM ahead of the World Economic Forum summit in Geneva."
    threshold: 0.5
  spacy_model: "en_core_web_sm"
  report:
    epoch:
      metrics: False
      summary: False
      clustering: False
      samples:
        num: 10
        show: False
        per_sentence_stats: False
    final:
      metrics: False
      summary: False
      clustering: False
      samples:
        num: 10
        show: False
        per_sentence_stats: False
  partition_dual:
    use: False
    distance: "cosine"   # options: cosine, euclidean
    cache:
      enable: True
      path: "data/partition_cache.json"
      max_length: 12

device: "cuda"

logging:
  metrics_only: False

dora:
  exclude: ["train.retrain", "eval.report.*", "eval.eval_only", "eval.verbose", "data.rebuild_ds"]

array: 0

slurm:
  partition: "boost_usr_prod"
  gpus: 1
  mem_per_gpu: 32
  time: 240          # minutes
  cpus_per_gpu: 8
  setup:
    - "module load python/3.11.7"
    - "source $HOME/RatCon/.venv/bin/activate"
    - "export SLURM_CPU_BIND=none"
    - "export HF_HOME=$HOME/hf-cache"
    - "export TRANSFORMERS_CACHE=$HF_HOME/transformers"
    - "export HF_DATASETS_CACHE=$HF_HOME/datasets"
    - "export HF_HUB_OFFLINE=1"
    - "export TRANSFORMERS_OFFLINE=1"
