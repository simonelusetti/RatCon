defaults:
  - _self_
  - override: default

model:
  encoder: 
    family: "sbert" # vanilla | sbert | retrieval | late
    name: "sentence-transformers/all-MiniLM-L6-v2"
  loss:
    l_rec: 1.0
    l_s: 100.0
    l_c: 1.0
    sparsity_target: 0.5
    tau: 0.07
    fourier:
      filter_mode: "none"
      cutoff: 0.5
  optim:
    lr: 5e-5
    weight_decay: 1e-2
    lr_adv: 5e-5
    weight_decay_adv: 1e-2
    betas: [0.9, 0.999]

data:
  runtime:
    rebuild: false
    batch_size: 256
    num_workers: 8
  max_length: 512
  dataset: "mr"
  config: null
  subset: 1.0
  tokenizer_name: "sentence-transformers/all-MiniLM-L6-v2"
  encoder: 
    family: "sbert" # vanilla | sbert | retrieval | late
    name: "sentence-transformers/all-MiniLM-L6-v2"


train:
  eval:
    eval_only: false
    eval_epoch: true
    examples: 10
  epochs: 30
  lr: 1e-3
  weight_decay: 0.0
  grad_clip: 1.0
  checkpoint: "model.pth"

runtime:
  num_threads: 8
  device: "cuda"
  token_parallelism: false
  global_log: false
  epoch_log: true

dora:
  exclude:
    - "data.runtime.*"
    - "slurm.*"
    - "runtime.*"
    - "train.eval.*"

array: 0

slurm:
  partition: "boost_usr_prod"
  gpus: 1
  mem_per_gpu: 32
  time: 240
  cpus_per_gpu: 8
  setup:
    - "module load python/3.11.7"
    - "source $HOME/RatCon/.venv/bin/activate"
    - "export SLURM_CPU_BIND=none"
    - "export HF_HOME=$HOME/hf-cache"
    - "export TRANSFORMERS_CACHE=$HF_HOME/transformers"
    - "export HF_DATASETS_CACHE=$HF_HOME/datasets"
    - "export HF_HUB_OFFLINE=1"
    - "export TRANSFORMERS_OFFLINE=1"
